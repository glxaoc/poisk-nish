"""
–ú–æ–¥—É–ª—å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
–ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å—ã –ø–æ —Å–º—ã—Å–ª—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
"""

from typing import List, Dict, Set
from collections import defaultdict
import re

# –°—Ç–æ–ø-—Å–ª–æ–≤–∞ ‚Äî —É–±–∏—Ä–∞–µ–º –∏–∑ –∞–Ω–∞–ª–∏–∑–∞
STOP_WORDS = {
    # –ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ
    '–∫—É–ø–∏—Ç—å', '–∫—É–ø–ª—é', '–ø–æ–∫—É–ø–∫–∞', '–∑–∞–∫–∞–∑–∞—Ç—å', '–∑–∞–∫–∞–∑',
    '—Ü–µ–Ω–∞', '—Ü–µ–Ω—ã', '—Ü–µ–Ω–æ–π', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Å—Ç–æ–∏—Ç',
    '–Ω–µ–¥–æ—Ä–æ–≥–æ', '–¥–µ—à–µ–≤–æ', '–¥—ë—à–µ–≤–æ', '–¥–µ—à–µ–≤—ã–π', '–¥–µ—à—ë–≤—ã–π',
    '—Å–∫–∏–¥–∫–∞', '—Å–∫–∏–¥–∫–∏', '–∞–∫—Ü–∏—è', '—Ä–∞—Å–ø—Ä–æ–¥–∞–∂–∞',
    '–¥–æ—Å—Ç–∞–≤–∫–∞', '–¥–æ—Å—Ç–∞–≤–∫–æ–π',
    
    # –ì–¥–µ –∫—É–ø–∏—Ç—å
    '–º–∞–≥–∞–∑–∏–Ω', '–º–∞–≥–∞–∑–∏–Ω–µ', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç',
    '–æ–Ω–ª–∞–π–Ω', '—Å–∞–π—Ç', '–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π',
    
    # –ü—Ä–µ–¥–ª–æ–≥–∏ –∏ —Å–æ—é–∑—ã
    '–≤', '–Ω–∞', '—Å', '—Å–æ', '–∏–∑', '–æ—Ç', '–¥–æ', '–¥–ª—è', '–ø–æ', '–∑–∞', '–∫', '—É',
    '–∏', '–∏–ª–∏', '–∞', '–Ω–æ', '—á—Ç–æ', '–∫–∞–∫', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–∫–∞–∫–æ–π', '–∫–∞–∫–∞—è', '–∫–∞–∫–∏–µ',
    
    # –ú–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è
    '–º–Ω–µ', '–º–æ–π', '–º–æ—è', '–º–æ–∏', '—Å–≤–æ–π', '—Å–≤–æ—è', '—Å–≤–æ–∏',
    
    # –ü—Ä–æ—á–µ–µ
    '–º–æ–∂–Ω–æ', '–Ω—É–∂–Ω–æ', '–ª—É—á—à–µ', '–ª—É—á—à–∏–π', '–ª—É—á—à–∏–µ', '—Ö–æ—Ä–æ—à–∏–π', '—Ö–æ—Ä–æ—à–∞—è', '—Ö–æ—Ä–æ—à–∏–µ',
    '–Ω–æ–≤—ã–π', '–Ω–æ–≤–∞—è', '–Ω–æ–≤—ã–µ', '2024', '2025', '2026',
    '–≥–æ–¥', '–≥–æ–¥–∞', '–≥–æ–¥—É',
}

# –ì–æ—Ä–æ–¥–∞ ‚Äî –≤—ã–¥–µ–ª—è–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä "–ì–µ–æ–≥—Ä–∞—Ñ–∏—è"
CITIES = {
    '–º–æ—Å–∫–≤–∞', '–º–æ—Å–∫–≤–µ', '–º–æ—Å–∫–æ–≤—Å–∫–∏–π', '–º—Å–∫',
    '—Å–ø–±', '–ø–∏—Ç–µ—Ä', '–ø–µ—Ç–µ—Ä–±—É—Ä–≥', '—Å–∞–Ω–∫—Ç',
    '–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫', '–µ–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥', '–∫–∞–∑–∞–Ω—å', '–∫—Ä–∞—Å–Ω–æ–¥–∞—Ä',
    '–Ω–∏–∂–Ω–∏–π', '—Å–∞–º–∞—Ä–∞', '–æ–º—Å–∫', '—Ä–æ—Å—Ç–æ–≤', '–≤–æ—Ä–æ–Ω–µ–∂', '–ø–µ—Ä–º—å',
    '–≤–æ–ª–≥–æ–≥—Ä–∞–¥', '–∫—Ä–∞—Å–Ω–æ—è—Ä—Å–∫', '—É—Ñ–∞', '—á–µ–ª—è–±–∏–Ω—Å–∫', '—Ç—é–º–µ–Ω—å',
}

# –ú–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å—ã ‚Äî –æ—Ç–¥–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä "–ü–ª–æ—â–∞–¥–∫–∏"
MARKETPLACES = {
    'wildberries', '–≤–∞–π–ª–¥–±–µ—Ä—Ä–∏–∑', '–≤–±', 'wb',
    'ozon', '–æ–∑–æ–Ω',
    '—è–Ω–¥–µ–∫—Å', '–º–∞—Ä–∫–µ—Ç', 'yandex',
    '–∞–≤–∏—Ç–æ', 'avito',
    'aliexpress', '–∞–ª–∏—ç–∫—Å–ø—Ä–µ—Å—Å', '–∞–ª–∏',
}


def extract_keywords(phrase: str) -> List[str]:
    """–ò–∑–≤–ª–µ—á—å –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ñ—Ä–∞–∑—ã"""
    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º
    words = re.findall(r'[–∞-—è—ëa-z0-9]+', phrase.lower())
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
    keywords = [w for w in words if w not in STOP_WORDS and len(w) > 2]
    
    return keywords


def detect_category(phrase: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Ñ—Ä–∞–∑—ã"""
    phrase_lower = phrase.lower()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –≥–æ—Ä–æ–¥–∞
    for city in CITIES:
        if city in phrase_lower:
            return "geo"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å—ã
    for mp in MARKETPLACES:
        if mp in phrase_lower:
            return "marketplace"
    
    return "other"


def clusterize(queries: List[Dict], root_phrase: str = "") -> Dict:
    """
    –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    
    Args:
        queries: –°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ [{"phrase": "...", "count": N}, ...]
        root_phrase: –ö–æ—Ä–Ω–µ–≤–∞—è —Ñ—Ä–∞–∑–∞ (–µ—ë —Å–ª–æ–≤–∞ —Ç–æ–∂–µ –∏—Å–∫–ª—é—á–∞–µ–º)
    
    Returns:
        –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
    """
    # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª–æ–≤–∞ –∫–æ—Ä–Ω–µ–≤–æ–π —Ñ—Ä–∞–∑—ã –≤ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
    root_words = set(extract_keywords(root_phrase))
    local_stop_words = STOP_WORDS | root_words
    
    # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å–ª–æ–≤–∞–º
    word_stats = defaultdict(lambda: {"count": 0, "phrases": []})
    
    # –û—Ç–¥–µ–ª—å–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –¥–ª—è –≥–µ–æ –∏ –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–æ–≤
    geo_cluster = {"count": 0, "phrases": []}
    marketplace_cluster = {"count": 0, "phrases": []}
    
    total_count = 0
    
    for q in queries:
        phrase = q.get("phrase", "")
        count = q.get("count", 0)
        total_count += count
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
        category = detect_category(phrase)
        
        if category == "geo":
            geo_cluster["count"] += count
            geo_cluster["phrases"].append({"phrase": phrase, "count": count})
            continue
        
        if category == "marketplace":
            marketplace_cluster["count"] += count
            marketplace_cluster["phrases"].append({"phrase": phrase, "count": count})
            continue
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        words = re.findall(r'[–∞-—è—ëa-z0-9]+', phrase.lower())
        keywords = [w for w in words if w not in local_stop_words and len(w) > 2]
        
        # –°—á–∏—Ç–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞
        for word in keywords:
            word_stats[word]["count"] += count
            word_stats[word]["phrases"].append({"phrase": phrase, "count": count})
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã –∏–∑ —Ç–æ–ø —Å–ª–æ–≤
    clusters = []
    used_phrases = set()
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Å–ª–æ–≤–∞ –ø–æ —Å—É–º–º–∞—Ä–Ω–æ–π —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏
    sorted_words = sorted(word_stats.items(), key=lambda x: x[1]["count"], reverse=True)
    
    for word, data in sorted_words[:20]:  # –¢–æ–ø-20 –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        # –§–∏–ª—å—Ç—Ä—É–µ–º —É–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã
        unique_phrases = [p for p in data["phrases"] if p["phrase"] not in used_phrases]
        
        if not unique_phrases:
            continue
        
        cluster_count = sum(p["count"] for p in unique_phrases)
        
        if cluster_count < total_count * 0.01:  # –ú–∏–Ω–∏–º—É–º 1% –æ—Ç –æ–±—â–µ–≥–æ
            continue
        
        clusters.append({
            "name": word,
            "count": cluster_count,
            "share": round(cluster_count / total_count * 100, 1) if total_count > 0 else 0,
            "phrases": sorted(unique_phrases, key=lambda x: x["count"], reverse=True)[:10]
        })
        
        # –ü–æ–º–µ—á–∞–µ–º —Ñ—Ä–∞–∑—ã –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ
        for p in unique_phrases:
            used_phrases.add(p["phrase"])
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≥–µ–æ-–∫–ª–∞—Å—Ç–µ—Ä –µ—Å–ª–∏ –µ—Å—Ç—å
    if geo_cluster["count"] > 0:
        geo_cluster["name"] = "üåç –ì–µ–æ–≥—Ä–∞—Ñ–∏—è"
        geo_cluster["share"] = round(geo_cluster["count"] / total_count * 100, 1) if total_count > 0 else 0
        geo_cluster["phrases"] = sorted(geo_cluster["phrases"], key=lambda x: x["count"], reverse=True)[:10]
        clusters.append(geo_cluster)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å-–∫–ª–∞—Å—Ç–µ—Ä –µ—Å–ª–∏ –µ—Å—Ç—å
    if marketplace_cluster["count"] > 0:
        marketplace_cluster["name"] = "üõí –ú–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å—ã"
        marketplace_cluster["share"] = round(marketplace_cluster["count"] / total_count * 100, 1) if total_count > 0 else 0
        marketplace_cluster["phrases"] = sorted(marketplace_cluster["phrases"], key=lambda x: x["count"], reverse=True)[:10]
        clusters.append(marketplace_cluster)
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã –ø–æ –¥–æ–ª–µ
    clusters = sorted(clusters, key=lambda x: x["count"], reverse=True)
    
    return {
        "total_count": total_count,
        "total_queries": len(queries),
        "clusters": clusters,
        "clusters_count": len(clusters)
    }


if __name__ == "__main__":
    # –¢–µ—Å—Ç
    test_queries = [
        {"phrase": "–∫—É–ø–∏—Ç—å —Å–∞–º–æ–∫–∞—Ç", "count": 28897},
        {"phrase": "–∫—É–ø–∏—Ç—å —Å–∞–º–æ–∫–∞—Ç —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏–π", "count": 5000},
        {"phrase": "–∫—É–ø–∏—Ç—å —Å–∞–º–æ–∫–∞—Ç –¥–µ—Ç—Å–∫–∏–π", "count": 3000},
        {"phrase": "–∫—É–ø–∏—Ç—å —Å–∞–º–æ–∫–∞—Ç –≤ –º–æ—Å–∫–≤–µ", "count": 2000},
        {"phrase": "–∫—É–ø–∏—Ç—å —Å–∞–º–æ–∫–∞—Ç –Ω–∞ wildberries", "count": 1500},
    ]
    
    result = clusterize(test_queries, "–∫—É–ø–∏—Ç—å —Å–∞–º–æ–∫–∞—Ç")
    print(f"–í—Å–µ–≥–æ: {result['total_count']:,} –∑–∞–ø—Ä–æ—Å–æ–≤")
    print(f"–ö–ª–∞—Å—Ç–µ—Ä–æ–≤: {result['clusters_count']}")
    for c in result["clusters"]:
        print(f"  {c['name']}: {c['count']:,} ({c['share']}%)")
